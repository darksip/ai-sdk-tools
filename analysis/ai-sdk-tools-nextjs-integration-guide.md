# Guide d'int√©gration AI SDK Tools dans Next.js + AI SDK v5

## üéØ Vue d'ensemble

Ce document analyse l'int√©r√™t de chaque biblioth√®que AI SDK Tools dans le contexte d'un projet **Next.js utilisant d√©j√† AI SDK v5 avec des agents impl√©ment√©s**.

### Contexte de d√©part assum√©

- **Framework**: Next.js 14/15 (App Router ou Pages Router)
- **AI SDK**: v5.x d√©j√† install√© et configur√©
- **Agents**: Impl√©mentations existantes bas√©es sur `generateText` / `streamText`
- **Stack typique**: API routes + React components
- **Infrastructure**: Serveur Node.js ou edge runtime

---

## üì¶ Analyse par package

### 1. @ai-sdk-tools/store

**Version**: 0.8.2
**Type**: 100% CLIENT (React hooks)
**D√©pendances**: `react ^18.0`, `zustand ^5.0`, `@ai-sdk/react ^2.0`

#### ‚ùì Quel probl√®me r√©sout-il ?

Le hook `useChat` vanilla d'AI SDK v5 (`@ai-sdk/react`) pr√©sente des limitations en production:

| Probl√®me vanilla AI SDK | Solution avec @ai-sdk-tools/store |
|-------------------------|-----------------------------------|
| Re-renders massifs sur chaque message stream | Batched updates + throttling (16ms) |
| Pas d'indexation des messages | MessageIndex O(1) lookups |
| √âtat volatil (perdu au refresh) | Hydration depuis sessionStorage/localStorage |
| Pas de DevTools | Middleware Zustand DevTools int√©gr√© |
| D√©tection de freeze UI difficile | Freeze detection automatique (>80ms warning) |
| S√©lecteurs non optimis√©s | useShallow pour √©viter re-renders inutiles |

#### üéØ Cas d'usage dans votre projet Next.js

**Sc√©nario 1: Chat avec historique persistant**
```tsx
// Avant (vanilla AI SDK v5)
'use client'
import { useChat } from '@ai-sdk/react'

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat()
  // ‚ùå √âtat perdu au refresh
  // ‚ùå Re-renders sur chaque token stream√©
  // ‚ùå Pas d'acc√®s √† l'√©tat dans d'autres composants
}

// Apr√®s (avec store)
'use client'
import { useChat, Provider } from '@ai-sdk-tools/store'

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    id: 'main-chat' // √âtat persistant li√© √† cet ID
  })
  // ‚úÖ √âtat persist√© automatiquement
  // ‚úÖ Performance optimis√©e
  // ‚úÖ Accessible globalement via hooks
}
```

**Sc√©nario 2: Dashboard avec affichage message dans plusieurs composants**
```tsx
// Sans store: prop drilling ou context lourd
<ChatMessages messages={messages} />
<MessageStats messages={messages} />
<LastMessage message={messages[messages.length - 1]} />

// Avec store: acc√®s direct depuis n'importe quel composant
'use client'
import { useChatMessages, useMessageCount } from '@ai-sdk-tools/store'

function ChatMessages() {
  const messages = useChatMessages() // Auto-subscribed
}

function MessageStats() {
  const count = useMessageCount() // Pas de re-render si messages changent
}
```

#### ‚öñÔ∏è Int√©r√™t vs vanilla AI SDK v5

**Adoptez store si:**
- ‚úÖ Vous avez plus de 50+ messages dans une conversation (performance)
- ‚úÖ Vous affichez les messages dans plusieurs composants
- ‚úÖ Vous voulez persister l'historique entre refreshes
- ‚úÖ Vous devez g√©rer plusieurs conversations simultan√©es
- ‚úÖ Vous souhaitez debugger l'√©tat facilement

**Restez vanilla AI SDK si:**
- ‚ùå Chat simple, √©ph√©m√®re, < 20 messages
- ‚ùå Pas besoin de persistence
- ‚ùå Pas de contraintes de performance
- ‚ùå Vous voulez √©viter une d√©pendance suppl√©mentaire

#### üîß Migration depuis vanilla AI SDK

**Drop-in replacement** - API 100% compatible:

```tsx
// Il suffit de changer l'import
- import { useChat } from '@ai-sdk/react'
+ import { useChat } from '@ai-sdk-tools/store'

// API identique, fonctionnalit√©s en plus
const chat = useChat({
  api: '/api/chat',
  id: 'my-chat' // Nouveau: persist avec cet ID
})
```

#### ‚ö†Ô∏è Points d'attention

1. **Hydration SSR**: Store est client-only, wrapper avec `<Provider>`
2. **Storage quota**: sessionStorage limit√© (~5MB), pr√©voir cleanup
3. **Zustand peer dep**: Doit √™tre install√© explicitement (`zustand ^5.0`)

---

### 2. @ai-sdk-tools/memory

**Version**: 0.1.2
**Type**: 100% SERVER (providers Redis/SQL)
**D√©pendances**: `zod ^4.1`
**Peer deps optionnelles**: `@upstash/redis ^1.34`, `drizzle-orm ^0.36`

#### ‚ùì Quel probl√®me r√©sout-il ?

AI SDK v5 vanilla **ne fournit pas** de syst√®me de m√©moire persistante. Vos agents oublient tout entre les requ√™tes.

| Sans memory | Avec @ai-sdk-tools/memory |
|-------------|---------------------------|
| Agents stateless | Working memory + chat history |
| Contexte perdu entre sessions | Persistence Redis/PostgreSQL/MySQL/SQLite |
| Pas de rappel de pr√©f√©rences utilisateur | M√©moire scop√©e par user/chat |
| Re-fetching √† chaque requ√™te | Cache de contexte |

#### üéØ Cas d'usage dans votre projet Next.js

**Sc√©nario 1: Agent avec m√©moire de conversation**

```typescript
// app/api/chat/route.ts
import { InMemoryProvider } from '@ai-sdk-tools/memory'
import { streamText } from 'ai'
import { openai } from '@ai-sdk/openai'

const memory = new InMemoryProvider()

export async function POST(req: Request) {
  const { messages, userId, sessionId } = await req.json()

  // R√©cup√©rer l'historique de conversation
  const history = await memory.getConversationHistory({
    sessionId,
    userId,
    limit: 50
  })

  const result = streamText({
    model: openai('gpt-4o'),
    messages: [
      { role: 'system', content: 'Tu es un assistant qui se souvient des conversations' },
      ...history.messages, // Contexte des conversations pr√©c√©dentes
      ...messages
    ]
  })

  // Sauvegarder apr√®s g√©n√©ration
  const { text } = await result.value
  await memory.saveMessage({
    sessionId,
    userId,
    message: { role: 'assistant', content: text }
  })

  return result.toDataStreamResponse()
}
```

**Sc√©nario 2: Working memory cross-requests**

```typescript
// Agent financier qui accumule des insights
const workingMemory = await memory.getWorkingMemory({
  scope: 'user',
  id: userId,
  agentName: 'financial-advisor'
})

// Structure: { insights: string[], preferences: {...} }
const insights = workingMemory?.insights || []

// Ajouter un nouveau insight
await memory.saveWorkingMemory({
  scope: 'user',
  id: userId,
  agentName: 'financial-advisor',
  memory: {
    insights: [...insights, 'User prefers conservative investments'],
    preferences: { riskTolerance: 'low' }
  }
})
```

**Sc√©nario 3: Production avec Upstash Redis**

```typescript
// lib/memory.ts
import { UpstashMemoryProvider } from '@ai-sdk-tools/memory'
import { Redis } from '@upstash/redis'

export const memory = new UpstashMemoryProvider({
  redis: Redis.fromEnv() // UPSTASH_REDIS_REST_URL + TOKEN
})

// TTL automatique:
// - Chat scope: 24h
// - User scope: 30 jours
// - Auto-trim des messages (dernier 100)
```

#### ‚öñÔ∏è Int√©r√™t vs impl√©mentation custom

**Adoptez memory si:**
- ‚úÖ Vous devez persister des conversations multi-sessions
- ‚úÖ Vos agents ont besoin de contexte utilisateur entre requ√™tes
- ‚úÖ Vous voulez une abstraction propre (3 providers ready-to-use)
- ‚úÖ Vous g√©rez plusieurs agents avec m√©moires isol√©es

**Impl√©mentez vous-m√™me si:**
- ‚ùå Vous avez d√©j√† un syst√®me de persistence custom bien √©tabli
- ‚ùå Besoin de logique de m√©moire tr√®s sp√©cifique (RAG, vector search)
- ‚ùå Projet simple sans besoin de m√©moire cross-sessions

#### üîß Int√©gration avec vos agents existants

```typescript
// Avant: agent stateless
export async function POST(req: Request) {
  const { messages } = await req.json()

  const result = streamText({
    model: openai('gpt-4o'),
    messages // Pas de contexte au-del√† de cette requ√™te
  })
}

// Apr√®s: agent avec m√©moire
import { memory } from '@/lib/memory'

export async function POST(req: Request) {
  const { messages, sessionId, userId } = await req.json()

  // Charger m√©moire working + historique
  const [workingMem, history] = await Promise.all([
    memory.getWorkingMemory({ scope: 'user', id: userId, agentName: 'assistant' }),
    memory.getConversationHistory({ sessionId, userId, limit: 20 })
  ])

  const systemContext = `
    Pr√©f√©rences utilisateur: ${JSON.stringify(workingMem)}
    Historique: ${history.messages.length} messages
  `

  const result = streamText({
    model: openai('gpt-4o'),
    messages: [
      { role: 'system', content: systemContext },
      ...messages
    ]
  })

  // Persister apr√®s g√©n√©ration
  const { text } = await result.value
  await memory.saveMessage({
    sessionId, userId,
    message: { role: 'assistant', content: text }
  })
}
```

#### ‚ö†Ô∏è Points d'attention

1. **Drizzle version**: Dev utilise 0.44.6, peer dep d√©clare 0.36.0 (incompatibilit√© possible)
2. **Schemas s√©par√©s**: Utiliser helpers `drizzle-schema.ts` pour PostgreSQL/MySQL/SQLite
3. **TTL Redis**: Messages auto-expir√©s, pr√©voir archivage long-terme si besoin
4. **Co√ªts**: Upstash Redis factur√© au volume, InMemory OK pour dev uniquement

---

### 3. @ai-sdk-tools/cache

**Version**: 0.7.2
**Type**: 100% SERVER (wrapper de tools)
**D√©pendances**: Aucune (peer: `ai ^5.0`)
**Backends**: LRU in-memory, Redis (Upstash, ioredis, redis)

#### ‚ùì Quel probl√®me r√©sout-il ?

AI SDK v5 **n'a pas de cache natif** pour les tools. Chaque appel co√ªte temps + argent, m√™me si les param√®tres sont identiques.

| Sans cache | Avec @ai-sdk-tools/cache |
|------------|-------------------------|
| Appel API √† chaque fois | Cache intelligent avec TTL |
| Co√ªts 10x+ pour requ√™tes r√©p√©t√©es | R√©duction ~80% des co√ªts |
| Latence √©lev√©e | R√©ponse instantan√©e (cache hit) |
| Pas de support streaming tools | Streaming tools + artifacts cach√©s |
| Gestion manuelle du cache | Cl√©s d√©terministes auto-g√©n√©r√©es |

#### üéØ Cas d'usage dans votre projet Next.js

**Sc√©nario 1: Tool API externe (m√©t√©o, traduction, taux de change)**

```typescript
// Avant: appel API √† chaque fois
import { tool } from 'ai'
import { z } from 'zod'

const weatherTool = tool({
  description: 'Get weather for a city',
  parameters: z.object({ city: z.string() }),
  execute: async ({ city }) => {
    const res = await fetch(`https://api.weather.com/${city}`) // ‚ùå Co√ªteux
    return res.json()
  }
})

// Apr√®s: cache Redis
import { createCached } from '@ai-sdk-tools/cache'
import { Redis } from '@upstash/redis'

const cached = createCached({
  cache: Redis.fromEnv(),
  ttl: 30 * 60 * 1000 // 30 min
})

const weatherTool = cached(tool({
  description: 'Get weather for a city',
  parameters: z.object({ city: z.string() }),
  execute: async ({ city }) => {
    const res = await fetch(`https://api.weather.com/${city}`)
    return res.json() // ‚úÖ R√©sultat cach√© 30min
  }
}))

// 1er appel: hit API
// Appels suivants (m√™me city): cache instantan√©
```

**Sc√©nario 2: Cache avec contexte multi-tenant**

```typescript
// Votre app a des teams isol√©es
const burnRateTool = tool({
  description: 'Analyze burn rate',
  parameters: z.object({ from: z.string(), to: z.string() }),
  execute: async ({ from, to }) => {
    const currentUser = getCurrentUser()
    return db.getBurnRate({
      teamId: currentUser.teamId, // ‚ö†Ô∏è Probl√®me: cache partag√© entre teams!
      from, to
    })
  }
})

// Solution: context-aware caching
import { cached } from '@ai-sdk-tools/cache'

const cachedBurnRate = cached(burnRateTool, {
  cacheKey: () => {
    const user = getCurrentUser()
    return `team:${user.teamId}:user:${user.id}` // ‚úÖ Isolation par team
  },
  ttl: 30 * 60 * 1000
})
```

**Sc√©nario 3: Streaming tools avec artifacts**

```typescript
// Cache fonctionne m√™me avec streaming + artifacts!
const analysisArtifact = artifact('analysis', schema)

const analysisTool = tool({
  description: 'Generate analysis',
  parameters: z.object({ companyId: z.string() }),
  execute: async function* ({ companyId }) {
    const report = analysisArtifact.stream({ stage: 'loading' })

    yield { text: 'Starting...' }
    await report.update({ stage: 'processing', data: [...] })
    yield { text: 'Complete', forceStop: true }
  }
})

const cachedAnalysis = cached(analysisTool)

// 1√®re ex√©cution: streaming complet + artifact cr√©√©
// Ex√©cutions suivantes: artifact restaur√© instantan√©ment + replay streaming
```

**Sc√©nario 4: Cache multiple tools**

```typescript
import { cacheTools } from '@ai-sdk-tools/cache'

const { weather, translation, exchange } = cacheTools({
  weather: weatherTool,
  translation: translationTool,
  exchange: exchangeRateTool
}, {
  ttl: 5 * 60 * 1000, // 5min pour tous
  debug: process.env.NODE_ENV === 'development'
})
```

#### ‚öñÔ∏è Int√©r√™t vs impl√©mentation custom

**Adoptez cache si:**
- ‚úÖ Vous avez des tools avec API externes co√ªteuses
- ‚úÖ Agents appellent les m√™mes tools avec m√™mes param√®tres
- ‚úÖ Vous voulez support streaming + artifacts out-of-the-box
- ‚úÖ Besoin de cache multi-tenant avec isolation

**Impl√©mentez vous-m√™me si:**
- ‚ùå Logique de cache tr√®s sp√©cifique (invalidation custom, warmup)
- ‚ùå Vous utilisez d√©j√† un cache centralis√© (ex: Redis avec structure custom)
- ‚ùå Tools toujours avec param√®tres diff√©rents (pas de r√©p√©tition)

#### üîß Monitoring et debugging

```typescript
const cachedTool = cached(originalTool, {
  onHit: (key) => {
    console.log(`‚ú® Cache HIT: ${key}`)
    analytics.track('tool_cache_hit', { key })
  },
  onMiss: (key) => {
    console.log(`üí´ Cache MISS: ${key}`)
    analytics.track('tool_cache_miss', { key })
  },
  debug: true // Logs d√©taill√©s
})

// Stats
console.log(cachedTool.getStats())
// { hits: 42, misses: 8, hitRate: 0.84, size: 50, maxSize: 1000 }

// Clear programmatiquement
cachedTool.clearCache() // Tout
cachedTool.clearCache('specific-key') // Cl√© pr√©cise
```

#### ‚ö†Ô∏è Points d'attention

1. **S√©rialisation cl√©s**: D√©terministe (ordre cl√©s tri√©) comme React Query
2. **TTL par d√©faut**: 10min LRU, 30min Redis (configurable)
3. **Artifacts**: N√©cessite `@ai-sdk-tools/artifacts` en peer dep optionnelle
4. **Streaming**: Seul le dernier chunk stock√© (texte complet), pas tous les deltas

---

### 4. @ai-sdk-tools/artifacts

**Version**: 0.8.2
**Type**: MIXTE (server pour `artifact()`, client pour `useArtifact`)
**D√©pendances**: `@ai-sdk-tools/store` (client), `zod`, `ai ^5.0`

#### ‚ùì Quel probl√®me r√©sout-il ?

AI SDK v5 supporte les **data parts** mais sans:
- Type safety via Zod
- √âtat de streaming (pending/streaming/complete/error)
- Progress tracking
- Hook React optimis√© pour consommation

| Vanilla AI SDK data parts | @ai-sdk-tools/artifacts |
|---------------------------|------------------------|
| Pas de validation sch√©ma | Zod schema strict |
| Statut manual tracking | Status automatique |
| Pas de progress API | `artifact.progress = 0.5` |
| Parsing manuel React | `useArtifact(schema)` optimis√© |
| Pas d'isolation multi-types | `useArtifacts()` pour tous types |

#### üéØ Cas d'usage dans votre projet Next.js

**Sc√©nario 1: Dashboard financier avec donn√©es structur√©es**

```typescript
// 1. D√©finir artifact (partag√© client/serveur)
// lib/artifacts.ts
import { artifact } from '@ai-sdk-tools/artifacts'
import { z } from 'zod'

export const burnRateArtifact = artifact('burn-rate', z.object({
  title: z.string(),
  stage: z.enum(['loading', 'processing', 'complete']),
  monthlyBurn: z.number(),
  runway: z.number(),
  chart: z.object({
    months: z.array(z.string()),
    values: z.array(z.number())
  }).optional()
}))

// 2. Tool server-side
// app/api/chat/route.ts
import { tool } from 'ai'
import { burnRateArtifact } from '@/lib/artifacts'

const analyzeBurnRate = tool({
  description: 'Analyze company burn rate',
  parameters: z.object({ companyId: z.string() }),
  execute: async function* ({ companyId }) {
    // Cr√©er artifact avec √©tat initial
    const analysis = burnRateArtifact.stream({
      title: `Analysis for ${companyId}`,
      stage: 'loading',
      monthlyBurn: 0,
      runway: 0
    })

    yield { text: 'Starting analysis...' }

    // Simuler √©tapes
    analysis.progress = 0.3
    const data = await fetchBurnRateData(companyId)

    await analysis.update({
      stage: 'processing',
      monthlyBurn: data.burn
    })

    yield { text: 'Calculating runway...' }
    analysis.progress = 0.7

    // Finaliser
    await analysis.complete({
      stage: 'complete',
      monthlyBurn: data.burn,
      runway: data.runway,
      chart: { months: [...], values: [...] }
    })

    yield { text: 'Analysis complete!', forceStop: true }
  }
})

// 3. Composant React
// components/BurnRateChart.tsx
'use client'
import { useArtifact } from '@ai-sdk-tools/artifacts/client'
import { burnRateArtifact } from '@/lib/artifacts'

export function BurnRateChart() {
  const { data, status, progress, error } = useArtifact(burnRateArtifact, {
    onComplete: (data) => {
      console.log('Analysis done!', data)
      analytics.track('burn_rate_complete')
    },
    onProgress: (p) => console.log(`Progress: ${p * 100}%`)
  })

  if (!data) return null
  if (error) return <Error message={error} />

  return (
    <div>
      <h2>{data.title}</h2>
      <p>Stage: {data.stage}</p>
      {progress && <ProgressBar value={progress} />}

      {status === 'streaming' && <Spinner />}

      {data.chart && (
        <LineChart
          months={data.chart.months}
          values={data.chart.values}
        />
      )}

      <div>
        Monthly Burn: ${data.monthlyBurn.toLocaleString()}
        Runway: {data.runway} months
      </div>
    </div>
  )
}
```

**Sc√©nario 2: Multiple artifact types (Canvas pattern)**

```typescript
// Plusieurs types d'artifacts
const chartArtifact = artifact('chart', chartSchema)
const tableArtifact = artifact('table', tableSchema)
const reportArtifact = artifact('report', reportSchema)

// Composant qui switch sur le type
'use client'
import { useArtifacts } from '@ai-sdk-tools/artifacts/client'

function Canvas() {
  const { current, latest } = useArtifacts({
    onData: (type, data) => {
      console.log(`New ${type}:`, data)
    }
  })

  // Afficher le dernier artifact re√ßu
  switch (current?.type) {
    case 'chart':
      return <ChartRenderer data={current.data} />
    case 'table':
      return <TableRenderer data={current.data} />
    case 'report':
      return <ReportRenderer data={current.data} />
    default:
      return <EmptyState />
  }
}
```

#### ‚öñÔ∏è Int√©r√™t vs vanilla data parts

**Adoptez artifacts si:**
- ‚úÖ Vous envoyez des donn√©es structur√©es complexes (tableaux, charts, objets)
- ‚úÖ Besoin de type safety strict (√©viter erreurs de parsing)
- ‚úÖ Vous voulez status + progress tracking automatique
- ‚úÖ Plusieurs types d'artifacts dans m√™me chat

**Utilisez vanilla data parts si:**
- ‚ùå Donn√©es simples (texte, nombres)
- ‚ùå Pas besoin de validation Zod
- ‚ùå Vous g√©rez le state manuellement
- ‚ùå Un seul type de donn√©e structur√©e

#### üîß Int√©gration avec agents existants

```typescript
// Avant: tool retourne JSON brut
const tool = tool({
  description: 'Generate chart',
  parameters: z.object({ metric: z.string() }),
  execute: async ({ metric }) => {
    const data = await getChartData(metric)
    return {
      type: 'chart', // ‚ùå Pas de validation
      data: data     // ‚ùå Pas de type safety
    }
  }
})

// Apr√®s: artifact typ√©
import { artifact } from '@ai-sdk-tools/artifacts'

const chartArtifact = artifact('chart', z.object({
  title: z.string(),
  series: z.array(z.object({
    name: z.string(),
    data: z.array(z.number())
  }))
}))

const tool = tool({
  description: 'Generate chart',
  parameters: z.object({ metric: z.string() }),
  execute: async function* ({ metric }) {
    const chart = chartArtifact.stream({
      title: metric,
      series: []
    })

    const data = await getChartData(metric)

    await chart.complete({
      title: metric,
      series: data // ‚úÖ Valid√© par Zod
    })

    yield { text: 'Chart ready', forceStop: true }
  }
})
```

#### ‚ö†Ô∏è Points d'attention

1. **Store requis**: Artifacts d√©pend de `@ai-sdk-tools/store` pour message management
2. **Provider React**: Wrapper app avec `<Provider>` du store
3. **Schemas partag√©s**: Importer artifact dans client ET serveur (monorepo friendly)
4. **Progress manual**: `artifact.progress = 0.5` doit √™tre set explicitement

---

### 5. @ai-sdk-tools/devtools

**Version**: 0.8.2
**Type**: 100% CLIENT (React component)
**D√©pendances**: `react ^18`, `@mui/material ^7`, `@xyflow/react ^12`, `@ai-sdk-tools/store` (optionnel)

#### ‚ùì Quel probl√®me r√©sout-il ?

AI SDK v5 **n'a pas de UI de debugging int√©gr√©e**. Debugger les streams, tools, agents n√©cessite `console.log` manuels.

| Sans devtools | Avec @ai-sdk-tools/devtools |
|---------------|----------------------------|
| console.log partout | UI d√©di√©e avec filtres |
| Pas de vue d'ensemble events | Timeline compl√®te des events |
| Debugging tool calls difficile | Inspection param√®tres + r√©sultats |
| Pas de metrics temps r√©el | Tokens/sec, chars/sec |
| √âtat Zustand invisible | Explorateur JSON du store |

#### üéØ Cas d'usage dans votre projet Next.js

**Sc√©nario 1: Debug chat Next.js en dev**

```tsx
// app/layout.tsx ou app/chat/page.tsx
'use client'
import { AIDevtools } from '@ai-sdk-tools/devtools'

export default function ChatPage() {
  return (
    <div>
      <ChatInterface />

      {/* Uniquement en dev */}
      {process.env.NODE_ENV === 'development' && (
        <AIDevtools
          maxEvents={1000}
          config={{
            position: 'bottom', // ou 'right', 'overlay'
            height: 400,
            streamCapture: {
              enabled: true,
              endpoint: '/api/chat',
              autoConnect: true
            }
          }}
        />
      )}
    </div>
  )
}
```

**Sc√©nario 2: Monitor tool calls**

```tsx
// DevTools capture automatiquement:
// - tool-call-start: nom, param√®tres, timestamp
// - tool-call-result: r√©sultat, dur√©e
// - tool-call-error: erreur, stack trace

// Filtrage dans l'UI:
// - Par type d'event
// - Par nom de tool
// - Search dans les donn√©es
```

**Sc√©nario 3: Debug multi-agents**

```tsx
// Avec int√©gration agent
const agent = new Agent({
  name: 'Financial Advisor',
  model: openai('gpt-4o'),
  onEvent: (event) => {
    // Events envoy√©s automatiquement au DevTools
    console.log('[Agent]', event)
  }
})

// DevTools affiche:
// - agent-start
// - agent-handoff (si multi-agents)
// - agent-step (tool calls)
// - agent-finish
```

**Sc√©nario 4: Monitor performance**

```tsx
<AIDevtools
  modelId="gpt-4o" // Pour calcul context window
  config={{
    throttle: {
      enabled: true,
      interval: 100, // Throttle text-delta √† 100ms
      includeTypes: ['text-delta']
    }
  }}
/>

// Metrics visibles:
// - Tokens/seconde
// - Chars/seconde
// - Context window utilization
// - Event timing
```

#### ‚öñÔ∏è Int√©r√™t vs console.log

**Adoptez devtools si:**
- ‚úÖ D√©veloppement actif avec debugging fr√©quent
- ‚úÖ Multi-agents / tools complexes
- ‚úÖ Vous utilisez d√©j√† `@ai-sdk-tools/store` (int√©gration bonus)
- ‚úÖ √âquipe qui debug ensemble (UI > logs)

**Restez console.log si:**
- ‚ùå Projet simple, peu de tools
- ‚ùå Vous ne voulez pas Material-UI (bundle lourd: 65KB CSS)
- ‚ùå Debugging occasionnel
- ‚ùå Edge runtime strict (devtools client-only)

#### üîß Int√©gration manuelle

```tsx
import { useAIDevtools } from '@ai-sdk-tools/devtools'

function CustomDebugPanel() {
  const {
    events,
    clearEvents,
    toggleCapturing,
    filterEvents,
    getEventStats
  } = useAIDevtools({
    maxEvents: 500,
    onEvent: (event) => {
      // Custom handling
      if (event.type === 'error') {
        Sentry.captureException(event.error)
      }
    }
  })

  const stats = getEventStats()
  const errors = filterEvents(['error'])

  return (
    <div>
      <p>Total events: {stats.total}</p>
      <p>Errors: {errors.length}</p>
      <button onClick={clearEvents}>Clear</button>
    </div>
  )
}
```

#### ‚ö†Ô∏è Points d'attention

1. **Bundle size**: Material-UI + Xyflow = ~200KB (uniquement charger en dev)
2. **Production**: TOUJOURS wrapper avec `process.env.NODE_ENV === 'development'`
3. **Store optionnel**: Fonctionne sans, mais pas de State tab
4. **Auto-capture**: Intercepte les streams AI SDK automatiquement

---

### 6. @ai-sdk-tools/agents

**Version**: 0.2.2
**Type**: 100% SERVER (orchestration)
**D√©pendances**: `@ai-sdk-tools/memory` (workspace), `ai ^5.0`, `zod ^3.25|^4.1`

#### ‚ùì Quel probl√®me r√©sout-il ?

AI SDK v5 vanilla n'a **pas de syst√®me multi-agents natif**. Vous devez impl√©menter manuellement:
- Routing entre agents
- Handoffs avec contexte
- Orchestration de workflows
- Tool permissions
- Guardrails

| Vanilla AI SDK agents | @ai-sdk-tools/agents |
|-----------------------|---------------------|
| Pas d'abstraction agent | Classe `Agent` avec config |
| Routing manuel | `matchOn` patterns + LLM routing |
| Handoffs custom | `handoffs: [agent1, agent2]` |
| Context passing manuel | Generic `<TContext>` typ√© |
| Pas de guardrails | Input/output validation int√©gr√©e |
| Pas de permissions tools | `permissions: { allowed: [...] }` |
| Pas de m√©moire int√©gr√©e | Memory provider optionnel |

#### üéØ Cas d'usage dans votre projet Next.js

**‚ö†Ô∏è Question cl√©**: Vous avez d√©j√† des agents AI SDK v5. **Devez-vous migrer vers @ai-sdk-tools/agents ?**

**Sc√©nario 1: Vous avez d√©j√† des agents simples (1-2 agents max)**

```typescript
// Votre impl√©mentation actuelle
export async function POST(req: Request) {
  const { messages, mode } = await req.json()

  let systemPrompt = ''
  let tools = {}

  if (mode === 'financial') {
    systemPrompt = 'You are a financial advisor'
    tools = { analyzeBurnRate, calculateROI }
  } else if (mode === 'technical') {
    systemPrompt = 'You are a technical expert'
    tools = { debugCode, reviewPR }
  }

  return streamText({
    model: openai('gpt-4o'),
    messages: [{ role: 'system', content: systemPrompt }, ...messages],
    tools
  }).toDataStreamResponse()
}
```

**‚úÖ Restez vanilla AI SDK** - Votre code est simple et fonctionne. Pas besoin de la complexit√© d'@ai-sdk-tools/agents.

**Sc√©nario 2: Vous avez des workflows multi-√©tapes avec handoffs**

```typescript
// Votre impl√©mentation actuelle (complexe!)
export async function POST(req: Request) {
  const { messages } = await req.json()

  // 1. Triage agent d√©cide du routage
  const triageResult = await generateText({
    model: openai('gpt-4o-mini'),
    messages,
    tools: {
      routeTo: tool({
        description: 'Route to specialist',
        parameters: z.object({ agent: z.enum(['technical', 'billing']) }),
        execute: async ({ agent }) => agent
      })
    }
  })

  const targetAgent = triageResult.toolCalls[0]?.args.agent

  // 2. Appeler agent sp√©cialis√©
  let specialistPrompt = ''
  let specialistTools = {}

  if (targetAgent === 'technical') {
    specialistPrompt = 'Technical support'
    specialistTools = technicalTools
  } else if (targetAgent === 'billing') {
    specialistPrompt = 'Billing support'
    specialistTools = billingTools
  }

  // 3. Ex√©cuter agent sp√©cialis√©
  const result = await streamText({
    model: openai('gpt-4o'),
    messages: [
      { role: 'system', content: specialistPrompt },
      ...messages,
      { role: 'assistant', content: `Routing to ${targetAgent}` }
    ],
    tools: specialistTools
  })

  return result.toDataStreamResponse()
}
```

**‚ùå Code verbeux, fragile, pas de type safety**

**‚úÖ Avec @ai-sdk-tools/agents:**

```typescript
import { Agent } from '@ai-sdk-tools/agents'
import { openai } from '@ai-sdk/openai'

// D√©finir agents sp√©cialis√©s
const technicalAgent = new Agent({
  name: 'Technical Support',
  model: openai('gpt-4o'),
  instructions: 'Handle technical issues',
  tools: technicalTools,
  matchOn: ['error', 'bug', 'crash', /technical/i] // Auto-routing
})

const billingAgent = new Agent({
  name: 'Billing Support',
  model: openai('gpt-4o'),
  instructions: 'Handle billing questions',
  tools: billingTools,
  matchOn: ['payment', 'invoice', 'subscription', /billing/i]
})

// Orchestrator
const triageAgent = new Agent({
  name: 'Triage',
  model: openai('gpt-4o-mini'), // Cheap model pour routing
  instructions: 'Route to appropriate specialist',
  handoffs: [technicalAgent, billingAgent] // Auto-cr√©√© handoff tools
})

// Route handler simplifi√©
export async function POST(req: Request) {
  const { messages } = await req.json()

  return triageAgent.toUIMessageStream({
    messages,
    maxRounds: 5, // Max 5 handoffs
    onEvent: (event) => {
      if (event.type === 'agent-handoff') {
        console.log(`Handoff: ${event.from} ‚Üí ${event.to}`)
      }
    }
  })
}
```

**Avantages:**
- ‚úÖ Code d√©claratif
- ‚úÖ Auto-routing via `matchOn` patterns
- ‚úÖ Type safety sur handoffs
- ‚úÖ Event tracking int√©gr√©
- ‚úÖ Max rounds pour √©viter boucles infinies

**Sc√©nario 3: Context typ√© cross-agents**

```typescript
// Votre usecase: agents ont besoin du contexte team/user
interface TeamContext {
  teamId: string
  userId: string
  permissions: string[]
  preferences: Record<string, any>
}

const dataAgent = new Agent<TeamContext>({
  name: 'Data Analyst',
  model: openai('gpt-4o'),
  instructions: (context) => `
    Analyzing for team ${context.teamId}.
    User: ${context.userId}
    Permissions: ${context.permissions.join(', ')}
  `,
  tools: (context) => ({
    // Tools peuvent aussi √™tre dynamiques!
    queryData: context.permissions.includes('read') ? readTool : null
  })
})

// Route avec context
export async function POST(req: Request) {
  const { messages } = await req.json()
  const session = await getSession(req)

  return dataAgent.toUIMessageStream({
    messages,
    context: {
      teamId: session.teamId,
      userId: session.userId,
      permissions: session.permissions,
      preferences: session.preferences
    }
  })
}
```

**Sc√©nario 4: Guardrails et permissions**

```typescript
const restrictedAgent = new Agent({
  name: 'Restricted Bot',
  model: openai('gpt-4o'),
  instructions: 'Help users',
  tools: {
    readData: readTool,
    writeData: writeTool,
    deleteData: deleteTool
  },
  permissions: {
    allowed: ['readData', 'writeData'], // deleteData bloqu√©
    maxCallsPerTool: {
      writeData: 5 // Max 5 writes
    }
  },
  inputGuardrails: [
    async (input) => {
      if (containsProfanity(input)) {
        return {
          pass: false,
          action: 'block',
          message: 'Input violates policy'
        }
      }
      return { pass: true }
    }
  ],
  outputGuardrails: [
    async (output) => {
      if (containsPII(output)) {
        return {
          pass: false,
          action: 'modify',
          modifiedOutput: redactPII(output)
        }
      }
      return { pass: true }
    }
  ]
})
```

#### ‚öñÔ∏è Int√©r√™t vs vanilla multi-agents

**Adoptez @ai-sdk-tools/agents si:**
- ‚úÖ Workflows multi-agents complexes (3+ agents)
- ‚úÖ Handoffs fr√©quents entre agents
- ‚úÖ Besoin de context typ√© cross-agents
- ‚úÖ Guardrails / permissions tools n√©cessaires
- ‚úÖ Vous voulez int√©grer memory (working memory + history)

**Restez vanilla AI SDK si:**
- ‚ùå 1-2 agents simples sans handoffs
- ‚ùå Logique de routing tr√®s custom (pas pattern matching)
- ‚ùå Vous pr√©f√©rez contr√¥le total bas-niveau
- ‚ùå Projet legacy avec orchestration custom bien √©tablie

#### üîß Migration progressive

```typescript
// √âtape 1: Wrapper agents existants sans changer logique
const existingAgent = new Agent({
  name: 'Legacy Agent',
  model: openai('gpt-4o'),
  instructions: yourExistingPrompt,
  tools: yourExistingTools
})

// Utilisation identique
const result = await existingAgent.generate({ prompt: 'Hello' })

// √âtape 2: Ajouter handoffs progressivement
const newSpecialist = new Agent({ ... })
existingAgent.handoffs = [newSpecialist]

// √âtape 3: Migrer vers context typ√©
existingAgent.instructions = (context) => buildPrompt(context)
```

#### ‚ö†Ô∏è Points d'attention

1. **Memory d√©pendance**: Agents d√©pend de `@ai-sdk-tools/memory` (workspace:*)
2. **Overhead**: Orchestration ajoute latence vs direct `streamText`
3. **Max turns**: D√©faut 10, configurer selon vos besoins
4. **Streaming UI**: `toUIMessageStream` pour Next.js, `stream()` pour autre usage

---

## üèóÔ∏è Architecture recommand√©e Next.js

### Stack compl√®te

```
Next.js App
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/chat/route.ts       ‚Üí Agents (server)
‚îÇ   ‚îî‚îÄ‚îÄ chat/page.tsx           ‚Üí useChat + Artifacts (client)
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îú‚îÄ‚îÄ agents.ts               ‚Üí Agent definitions (server)
‚îÇ   ‚îú‚îÄ‚îÄ artifacts.ts            ‚Üí Artifact schemas (shared)
‚îÇ   ‚îú‚îÄ‚îÄ tools.ts                ‚Üí Tool definitions avec cache (server)
‚îÇ   ‚îî‚îÄ‚îÄ memory.ts               ‚Üí Memory provider config (server)
‚îî‚îÄ‚îÄ components/
    ‚îú‚îÄ‚îÄ Chat.tsx                ‚Üí useChat from store (client)
    ‚îú‚îÄ‚îÄ BurnRateChart.tsx       ‚Üí useArtifact (client)
    ‚îî‚îÄ‚îÄ DevTools.tsx            ‚Üí AIDevtools (client, dev only)
```

### Exemple complet d'int√©gration

```typescript
// lib/memory.ts (server)
import { UpstashMemoryProvider } from '@ai-sdk-tools/memory'
import { Redis } from '@upstash/redis'

export const memory = new UpstashMemoryProvider({
  redis: Redis.fromEnv()
})

// lib/tools.ts (server)
import { tool } from 'ai'
import { createCached } from '@ai-sdk-tools/cache'
import { Redis } from '@upstash/redis'

const cached = createCached({ cache: Redis.fromEnv() })

export const weatherTool = cached(tool({
  description: 'Get weather',
  parameters: z.object({ city: z.string() }),
  execute: async ({ city }) => {
    const res = await fetch(`https://api.weather/${city}`)
    return res.json()
  }
}))

// lib/artifacts.ts (shared)
import { artifact } from '@ai-sdk-tools/artifacts'

export const reportArtifact = artifact('report', z.object({
  title: z.string(),
  content: z.string(),
  stage: z.enum(['pending', 'complete'])
}))

// lib/agents.ts (server)
import { Agent } from '@ai-sdk-tools/agents'
import { openai } from '@ai-sdk/openai'
import { weatherTool } from './tools'
import { memory } from './memory'

export const weatherAgent = new Agent({
  name: 'Weather Expert',
  model: openai('gpt-4o'),
  instructions: 'Provide weather information',
  tools: { weather: weatherTool },
  matchOn: ['weather', 'forecast', 'temperature']
})

// app/api/chat/route.ts (server)
import { weatherAgent } from '@/lib/agents'
import { memory } from '@/lib/memory'

export async function POST(req: Request) {
  const { messages, sessionId, userId } = await req.json()

  // Charger historique
  const history = await memory.getConversationHistory({
    sessionId, userId, limit: 20
  })

  const result = await weatherAgent.toUIMessageStream({
    messages: [...history.messages, ...messages],
    maxRounds: 3,
    onEvent: (event) => {
      console.log('[Agent Event]', event)
    }
  })

  // Sauvegarder apr√®s (TODO: hook dans agent)
  return result
}

// app/chat/page.tsx (client)
'use client'
import { useChat } from '@ai-sdk-tools/store'
import { useArtifact } from '@ai-sdk-tools/artifacts/client'
import { AIDevtools } from '@ai-sdk-tools/devtools'
import { reportArtifact } from '@/lib/artifacts'

export default function ChatPage() {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    api: '/api/chat',
    id: 'weather-chat'
  })

  const { data: report, status } = useArtifact(reportArtifact)

  return (
    <div>
      <div>
        {messages.map(m => (
          <div key={m.id}>{m.content}</div>
        ))}
      </div>

      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} />
        <button type="submit">Send</button>
      </form>

      {report && (
        <div>
          <h2>{report.title}</h2>
          <p>{report.content}</p>
          {status === 'streaming' && <Spinner />}
        </div>
      )}

      {process.env.NODE_ENV === 'development' && <AIDevtools />}
    </div>
  )
}
```

---

## üìä Tableau de d√©cision

| Package | Adoptez si... | √âvitez si... |
|---------|---------------|--------------|
| **store** | Conversations longues, multi-composants, persistence | Chat simple < 20 msgs, pas de persistence |
| **memory** | Contexte cross-sessions, working memory agents | Agents stateless, pas de sessions utilisateurs |
| **cache** | Tools API externes, requ√™tes r√©p√©t√©es | Tools toujours diff√©rents, latence non critique |
| **artifacts** | Donn√©es structur√©es complexes, progress tracking | Donn√©es simples texte/nombres |
| **devtools** | Dev actif, multi-agents complexes | Projet simple, bundle size critique |
| **agents** | 3+ agents, handoffs, workflows complexes | 1-2 agents simples, orchestration custom |

---

## ‚ö†Ô∏è Limitations et consid√©rations

### Performance

- **store**: Batching optimis√©, mais storage quota limit√© (~5MB sessionStorage)
- **memory**: Redis calls ajoutent latence (pr√©voir caching applicatif)
- **cache**: LRU in-memory limit√© par RAM serveur
- **artifacts**: Overhead validation Zod sur chaque update
- **devtools**: Material-UI bundle (200KB), uniquement dev
- **agents**: Orchestration ajoute latence vs direct streamText

### Co√ªts

- **memory (Upstash)**: Factur√© au volume Redis (watch out auto-trim)
- **cache (Redis)**: Stockage + requests factur√©s
- **agents**: LLM routing co√ªte tokens (utiliser gpt-4o-mini)

### Compatibilit√©

- **store**: Client-only, SSR n√©cessite Provider
- **memory**: Drizzle peer dep 0.36 vs dev 0.44 (breaking?)
- **cache**: N√©cessite artifacts en peer optionnelle pour streaming tools
- **artifacts**: D√©pend obligatoirement de store
- **devtools**: React 18+, pas compatible edge runtime
- **agents**: D√©pend de memory (workspace), Zod v3 ou v4

### Vendor lock-in

Tous les packages sont **MIT** et peuvent √™tre remplac√©s/fork√©s. Cependant:

- **store**: Abstraitement Zustand, migration vers autre state manager = r√©√©criture
- **memory**: Interface propre, facile √† r√©impl√©menter custom provider
- **cache**: Wrapper l√©ger, facile √† remplacer
- **artifacts**: Coupl√© √† store, migration = r√©√©criture
- **agents**: Classe Agent custom, migration vers framework tiers = r√©√©criture

---

## üéì Recommandations finales

### Adoption progressive conseill√©e

**Phase 1 - Quick wins (semaine 1)**
1. **cache** sur tools API externes ‚Üí ROI imm√©diat (co√ªts ‚Üì, latence ‚Üì)
2. **devtools** en dev ‚Üí Debug am√©lior√© sans refactor

**Phase 2 - Foundations (semaine 2-3)**
3. **store** pour chat principal ‚Üí Performance + persistence
4. **memory** pour historique conversations ‚Üí Contexte utilisateur

**Phase 3 - Advanced (mois 1+)**
5. **artifacts** pour dashboards/visualisations ‚Üí UX am√©lior√©e
6. **agents** pour workflows multi-agents ‚Üí Orchestration propre

### Packages standalone vs stack compl√®te

**Utilisez packages isol√©ment:**
- Cache seul pour optimiser tools existants
- Store seul pour am√©liorer chat UI
- Memory seul pour contexte agents

**Utilisez stack compl√®te si:**
- Nouveau projet from scratch
- Refactor complet d'app existante
- √âquipe d√©di√©e AI/LLM avec best practices

### Alternative: Package unifi√© `ai-sdk-tools`

```bash
npm install ai-sdk-tools
```

```typescript
// Server-side
import { Agent, artifact, cached } from 'ai-sdk-tools'

// Client-side
import { useChat, useArtifact, AIDevtools } from 'ai-sdk-tools/client'
```

**Avantage**: Un seul package, versions synchro
**Inconv√©nient**: Bundle size si vous n'utilisez pas tout

---

## üìö Ressources

- **Documentation**: README de chaque package (`packages/*/README.md`)
- **Exemples**: `/apps/example/src/ai/` (Next.js 15 real-world examples)
- **GitHub**: https://github.com/midday-ai/ai-sdk-tools
- **Issues**: https://github.com/midday-ai/ai-sdk-tools/issues

---

**Derni√®re mise √† jour**: Document bas√© sur versions d√©cembre 2024
