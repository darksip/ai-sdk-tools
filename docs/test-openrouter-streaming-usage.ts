#!/usr/bin/env bun
/**
 * Test de tracking d'usage avec OpenRouter en mode streaming
 *
 * Ce script v√©rifie que les m√©triques d'usage (tokens, co√ªts) sont correctement
 * track√©es lors de l'utilisation de streamText avec le provider OpenRouter.
 *
 * Usage:
 *   bun run test-openrouter-streaming-usage.ts
 *
 * Pr√©requis:
 *   - OPENROUTER_API_KEY d√©fini dans .env
 *   - @openrouter/ai-sdk-provider install√©
 *   - ai install√©
 */

import { openrouter } from '@openrouter/ai-sdk-provider'
import { streamText } from 'ai'

// Configuration
const TEST_MODEL = 'openai/gpt-3.5-turbo' // Mod√®le peu co√ªteux pour les tests
const TEST_PROMPT = 'Write a haiku about TypeScript'

interface UsageMetrics {
  promptTokens?: number
  completionTokens?: number
  totalTokens?: number
  cost?: number
  promptTokensDetails?: {
    cachedTokens?: number
  }
  completionTokensDetails?: {
    reasoningTokens?: number
  }
}

/**
 * Test 1: Streaming avec usage tracking activ√©
 */
async function testStreamingWithUsageTracking() {
  console.log('\nüß™ Test 1: Streaming avec usage tracking ACTIV√â')
  console.log('‚ïê'.repeat(60))

  const model = openrouter(TEST_MODEL, {
    usage: { include: true } // ‚úÖ Activer le tracking
  })

  let streamedText = ''
  let finishChunk: any = null
  let chunkCount = 0

  try {
    const result = streamText({
      model,
      prompt: TEST_PROMPT,
      onFinish: (event) => {
        finishChunk = event
        console.log('\nüìä Finish event re√ßu')
      }
    })

    // Consommer le stream
    console.log('\nüìù Streaming en cours...')
    for await (const chunk of result.textStream) {
      streamedText += chunk
      chunkCount++
      process.stdout.write('.')
    }

    console.log('\n\n‚úÖ Stream termin√©')
    console.log(`   Chunks re√ßus: ${chunkCount}`)
    console.log(`   Texte g√©n√©r√©: "${streamedText.trim()}"`)

    // V√©rifier les m√©triques d'usage
    if (finishChunk?.providerMetadata?.openrouter?.usage) {
      const usage: UsageMetrics = finishChunk.providerMetadata.openrouter.usage

      console.log('\nüìà M√©triques d\'usage:')
      console.log(`   Prompt tokens:     ${usage.promptTokens || 0}`)
      console.log(`   Completion tokens: ${usage.completionTokens || 0}`)
      console.log(`   Total tokens:      ${usage.totalTokens || 0}`)
      console.log(`   Co√ªt:              $${usage.cost?.toFixed(6) || 0}`)

      if (usage.promptTokensDetails?.cachedTokens) {
        console.log(`   Cached tokens:     ${usage.promptTokensDetails.cachedTokens}`)
      }

      if (usage.completionTokensDetails?.reasoningTokens) {
        console.log(`   Reasoning tokens:  ${usage.completionTokensDetails.reasoningTokens}`)
      }

      // Assertions
      console.log('\n‚úì V√©rifications:')
      console.log(`   ${usage.totalTokens! > 0 ? '‚úÖ' : '‚ùå'} Total tokens > 0`)
      console.log(`   ${usage.cost! >= 0 ? '‚úÖ' : '‚ùå'} Co√ªt >= 0`)
      console.log(`   ${usage.promptTokens! > 0 ? '‚úÖ' : '‚ùå'} Prompt tokens > 0`)
      console.log(`   ${usage.completionTokens! > 0 ? '‚úÖ' : '‚ùå'} Completion tokens > 0`)

      return {
        success: true,
        usage
      }
    } else {
      console.error('\n‚ùå ERREUR: Pas de m√©triques d\'usage dans finishChunk')
      console.error('   providerMetadata:', JSON.stringify(finishChunk?.providerMetadata, null, 2))
      return { success: false }
    }

  } catch (error) {
    console.error('\n‚ùå ERREUR lors du streaming:', error)
    return { success: false, error }
  }
}

/**
 * Test 2: Streaming SANS usage tracking
 */
async function testStreamingWithoutUsageTracking() {
  console.log('\nüß™ Test 2: Streaming SANS usage tracking')
  console.log('‚ïê'.repeat(60))

  const model = openrouter(TEST_MODEL)
  // ‚ö†Ô∏è Pas de { usage: { include: true } }

  let finishChunk: any = null

  try {
    const result = streamText({
      model,
      prompt: TEST_PROMPT,
      onFinish: (event) => {
        finishChunk = event
      }
    })

    // Consommer le stream
    for await (const chunk of result.textStream) {
      // Ignorer le texte
    }

    console.log('‚úÖ Stream termin√©')

    // V√©rifier l'absence de m√©triques d√©taill√©es
    const usage = finishChunk?.providerMetadata?.openrouter?.usage

    if (!usage || Object.keys(usage).length === 0) {
      console.log('‚úÖ Aucune m√©trique d\'usage (comportement attendu)')
      return { success: true }
    } else {
      console.log('‚ö†Ô∏è  Des m√©triques sont pr√©sentes (inattendu):')
      console.log(JSON.stringify(usage, null, 2))
      return { success: true, unexpected: usage }
    }

  } catch (error) {
    console.error('‚ùå ERREUR:', error)
    return { success: false, error }
  }
}

/**
 * Test 3: Comparaison de co√ªts entre mod√®les
 */
async function testCostComparison() {
  console.log('\nüß™ Test 3: Comparaison de co√ªts entre mod√®les')
  console.log('‚ïê'.repeat(60))

  const models = [
    'openai/gpt-3.5-turbo',
    'openai/gpt-4o-mini',
    'anthropic/claude-3-haiku'
  ]

  const results: Array<{
    model: string
    tokens: number
    cost: number
  }> = []

  for (const modelId of models) {
    console.log(`\nüìä Test de ${modelId}...`)

    const model = openrouter(modelId, {
      usage: { include: true }
    })

    try {
      // Utiliser onFinish pour capturer les m√©triques
      let capturedUsage: any = null

      const result = streamText({
        model,
        prompt: TEST_PROMPT,
        onFinish: (event) => {
          capturedUsage = event.providerMetadata?.openrouter?.usage
        }
      })

      // Consommer le stream
      for await (const _ of result.textStream) {
        // Ignorer
      }

      if (capturedUsage) {
        results.push({
          model: modelId,
          tokens: capturedUsage.totalTokens || 0,
          cost: capturedUsage.cost || 0
        })

        console.log(`   Tokens: ${capturedUsage.totalTokens}`)
        console.log(`   Co√ªt:   $${capturedUsage.cost?.toFixed(6)}`)
      } else {
        console.log(`   ‚ö†Ô∏è  Pas de m√©triques disponibles`)
      }

    } catch (error) {
      console.error(`   ‚ùå Erreur: ${error}`)
    }

    // Attendre un peu entre les appels
    await new Promise(resolve => setTimeout(resolve, 500))
  }

  // Afficher le tableau comparatif
  console.log('\nüìä Tableau comparatif:')
  console.log('‚îÄ'.repeat(60))
  console.log('Mod√®le'.padEnd(35), 'Tokens'.padEnd(10), 'Co√ªt')
  console.log('‚îÄ'.repeat(60))

  if (results.length > 0) {
    results
      .sort((a, b) => a.cost - b.cost)
      .forEach(r => {
        console.log(
          r.model.padEnd(35),
          r.tokens.toString().padEnd(10),
          `$${r.cost.toFixed(6)}`
        )
      })
  } else {
    console.log('Aucun r√©sultat disponible')
  }

  console.log('‚îÄ'.repeat(60))

  return { success: true, results }
}

/**
 * Test 4: Monitoring des co√ªts sur plusieurs appels
 */
async function testCostMonitoring() {
  console.log('\nüß™ Test 4: Monitoring des co√ªts')
  console.log('‚ïê'.repeat(60))

  const model = openrouter('openai/gpt-3.5-turbo', {
    usage: { include: true }
  })

  const prompts = [
    'Write a haiku about TypeScript',
    'Explain async/await in one sentence',
    'What is functional programming?'
  ]

  let totalCost = 0
  let totalTokens = 0

  console.log('\nüìä Ex√©cution de 3 requ√™tes...\n')

  for (let i = 0; i < prompts.length; i++) {
    const prompt = prompts[i]
    console.log(`${i + 1}. "${prompt}"`)

    try {
      // Capturer les m√©triques dans onFinish
      let capturedUsage: any = null

      const result = streamText({
        model,
        prompt,
        onFinish: (event) => {
          capturedUsage = event.providerMetadata?.openrouter?.usage
        }
      })

      // Consommer le stream
      let text = ''
      for await (const chunk of result.textStream) {
        text += chunk
      }

      if (capturedUsage) {
        totalCost += capturedUsage.cost || 0
        totalTokens += capturedUsage.totalTokens || 0

        console.log(`   ‚Üí Tokens: ${capturedUsage.totalTokens}, Co√ªt: $${capturedUsage.cost?.toFixed(6)}`)
      } else {
        console.log(`   ‚Üí Pas de m√©triques d'usage`)
      }

      // Petit d√©lai entre les appels
      await new Promise(resolve => setTimeout(resolve, 300))

    } catch (error) {
      console.error(`   ‚ùå Erreur: ${error}`)
    }
  }

  console.log('\n' + '‚îÄ'.repeat(60))
  console.log(`üìà Total: ${totalTokens} tokens, $${totalCost.toFixed(6)}`)

  if (prompts.length > 0 && totalTokens > 0) {
    console.log(`üìä Moyenne: ${Math.round(totalTokens / prompts.length)} tokens/requ√™te, $${(totalCost / prompts.length).toFixed(6)}/requ√™te`)
  }

  return {
    success: true,
    totalCost,
    totalTokens,
    requestCount: prompts.length
  }
}

/**
 * Main: Ex√©cuter tous les tests
 */
async function main() {
  console.log('üöÄ Test du tracking d\'usage OpenRouter en streaming')
  console.log('‚ïê'.repeat(60))

  // V√©rifier la cl√© API
  if (!process.env.OPENROUTER_API_KEY) {
    console.error('‚ùå ERREUR: OPENROUTER_API_KEY non d√©fini')
    console.error('   D√©finissez la variable d\'environnement:')
    console.error('   export OPENROUTER_API_KEY=sk-or-v1-...')
    process.exit(1)
  }

  const results = {
    test1: await testStreamingWithUsageTracking(),
    test2: await testStreamingWithoutUsageTracking(),
    test3: await testCostComparison(),
    test4: await testCostMonitoring()
  }

  // R√©sum√© final
  console.log('\n' + '‚ïê'.repeat(60))
  console.log('üìä R√âSUM√â DES TESTS')
  console.log('‚ïê'.repeat(60))

  const allSuccess = Object.values(results).every(r => r.success)

  console.log(`Test 1 (avec tracking):  ${results.test1.success ? '‚úÖ' : '‚ùå'}`)
  console.log(`Test 2 (sans tracking):  ${results.test2.success ? '‚úÖ' : '‚ùå'}`)
  console.log(`Test 3 (comparaison):    ${results.test3.success ? '‚úÖ' : '‚ùå'}`)
  console.log(`Test 4 (monitoring):     ${results.test4.success ? '‚úÖ' : '‚ùå'}`)

  console.log('\n' + (allSuccess ? '‚úÖ TOUS LES TESTS PASSENT' : '‚ùå CERTAINS TESTS ONT √âCHOU√â'))

  process.exit(allSuccess ? 0 : 1)
}

// Ex√©cuter
main().catch(error => {
  console.error('‚ùå Erreur fatale:', error)
  process.exit(1)
})
