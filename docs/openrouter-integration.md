# Int√©gration OpenRouter avec AI SDK v5

> **üìö Guide d'int√©gration**
>
> Ce document explique comment utiliser le provider OpenRouter avec AI SDK v5 dans vos applications, y compris avec les packages [@fondation-io/ai-sdk-tools](https://github.com/darksip/ai-sdk-tools).

> **üÜï Native Support Update**
>
> `@fondation-io/agents` now includes native OpenRouter utilities for type-safe usage tracking and budget monitoring!
> See the [Native Support Guide](/docs/guides/openrouter-native-support.md) for the simplified API.

## Vue d'ensemble

OpenRouter est un provider qui donne acc√®s √† **plus de 300 mod√®les de langage** via une API unifi√©e compatible avec AI SDK v5. Il permet d'utiliser des mod√®les de diff√©rents fournisseurs (OpenAI, Anthropic, Google, Meta, etc.) avec une seule cl√© API.

**Avantages**:
- ‚úÖ Acc√®s √† 300+ mod√®les via une seule API
- ‚úÖ Fallback automatique entre mod√®les
- ‚úÖ Tarification transparente et comp√©titive
- ‚úÖ Support complet d'AI SDK v5
- ‚úÖ Prompt caching (Anthropic)
- ‚úÖ Usage tracking int√©gr√©
- ‚úÖ **Type-safe utilities in @fondation-io/agents**

**Repository**: https://github.com/OpenRouterTeam/ai-sdk-provider
**NPM**: `@openrouter/ai-sdk-provider`
**License**: Apache-2.0

**Native Support**: See `/docs/guides/openrouter-native-support.md` for type-safe utilities and examples.

---

## Installation

### AI SDK v5 (version actuelle)

```bash
# npm
npm install @openrouter/ai-sdk-provider

# pnpm
pnpm add @openrouter/ai-sdk-provider

# yarn
yarn add @openrouter/ai-sdk-provider

# bun
bun add @openrouter/ai-sdk-provider
```

### AI SDK v4 (legacy)

Si vous utilisez encore AI SDK v4:

```bash
npm install @openrouter/ai-sdk-provider@ai-sdk-v4
```

---

## Configuration

### 1. Obtenir une cl√© API

1. Cr√©er un compte sur [openrouter.ai](https://openrouter.ai)
2. G√©n√©rer une cl√© API dans les settings
3. Ajouter √† vos variables d'environnement

```bash
# .env.local
OPENROUTER_API_KEY=sk-or-v1-...
```

### 2. Import et setup de base

```typescript
import { openrouter } from '@openrouter/ai-sdk-provider'
```

Le provider utilise automatiquement `process.env.OPENROUTER_API_KEY` si d√©fini.

### 3. Configuration personnalis√©e (optionnel)

```typescript
import { createOpenRouter } from '@openrouter/ai-sdk-provider'

const openrouter = createOpenRouter({
  apiKey: 'your-api-key', // Override env var
  baseURL: 'https://openrouter.ai/api/v1', // Custom endpoint
  headers: {
    'HTTP-Referer': 'https://yourapp.com',
    'X-Title': 'Your App Name'
  }
})
```

---

## Usage de base

### G√©n√©ration de texte simple

```typescript
import { openrouter } from '@openrouter/ai-sdk-provider'
import { generateText } from 'ai'

const { text } = await generateText({
  model: openrouter('openai/gpt-4o'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.'
})

console.log(text)
```

### Streaming de texte

```typescript
import { openrouter } from '@openrouter/ai-sdk-provider'
import { streamText } from 'ai'

const result = streamText({
  model: openrouter('anthropic/claude-3-5-sonnet'),
  prompt: 'Explain quantum computing in simple terms.'
})

for await (const chunk of result.textStream) {
  process.stdout.write(chunk)
}
```

### G√©n√©ration d'objets structur√©s

```typescript
import { openrouter } from '@openrouter/ai-sdk-provider'
import { generateObject } from 'ai'
import { z } from 'zod'

const { object } = await generateObject({
  model: openrouter('openai/gpt-4o'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.string()),
      steps: z.array(z.string())
    })
  }),
  prompt: 'Generate a recipe for chocolate chip cookies.'
})

console.log(object.recipe)
```

---

## Mod√®les disponibles

OpenRouter donne acc√®s √† des centaines de mod√®les. Voici les plus populaires:

### OpenAI

```typescript
openrouter('openai/gpt-4o')              // GPT-4 Omni
openrouter('openai/gpt-4o-mini')         // GPT-4 Omni Mini
openrouter('openai/gpt-4-turbo')         // GPT-4 Turbo
openrouter('openai/gpt-3.5-turbo')       // GPT-3.5 Turbo
openrouter('openai/o1-preview')          // O1 Preview
openrouter('openai/o1-mini')             // O1 Mini
```

### Anthropic

```typescript
openrouter('anthropic/claude-3-5-sonnet')        // Claude 3.5 Sonnet
openrouter('anthropic/claude-3-opus')            // Claude 3 Opus
openrouter('anthropic/claude-3-sonnet')          // Claude 3 Sonnet
openrouter('anthropic/claude-3-haiku')           // Claude 3 Haiku
```

### Google

```typescript
openrouter('google/gemini-pro-1.5')              // Gemini 1.5 Pro
openrouter('google/gemini-flash-1.5')            // Gemini 1.5 Flash
openrouter('google/gemini-2.0-flash-exp')        // Gemini 2.0 Flash (experimental)
```

### Meta

```typescript
openrouter('meta-llama/llama-3.3-70b-instruct')  // Llama 3.3 70B
openrouter('meta-llama/llama-3.1-405b-instruct') // Llama 3.1 405B
openrouter('meta-llama/llama-3.1-70b-instruct')  // Llama 3.1 70B
```

### Mistral

```typescript
openrouter('mistralai/mistral-large')            // Mistral Large
openrouter('mistralai/mistral-medium')           // Mistral Medium
openrouter('mistralai/mixtral-8x7b-instruct')    // Mixtral 8x7B
```

### Liste compl√®te

Voir tous les mod√®les disponibles sur: https://openrouter.ai/models

Filtrer les mod√®les supportant les tools: https://openrouter.ai/models?supports=tools

---

## Param√®tres avanc√©s

### M√©thode 1: Provider Options (par requ√™te)

Passer des param√®tres sp√©cifiques OpenRouter pour une requ√™te donn√©e:

```typescript
import { streamText } from 'ai'

const result = await streamText({
  model: openrouter('openai/gpt-4o'),
  messages: [
    { role: 'user', content: 'Explain relativity' }
  ],
  providerOptions: {
    openrouter: {
      // Param√®tres sp√©cifiques OpenRouter
      reasoning: { max_tokens: 10 },
      transforms: ['middle-out'],
      route: 'fallback',
      models: [
        'openai/gpt-4o',
        'anthropic/claude-3-5-sonnet'
      ]
    }
  }
})
```

**Options disponibles**:
- `reasoning`: Configuration du raisonnement (pour mod√®les O1)
- `transforms`: Transformations de requ√™te
- `route`: Strat√©gie de routage (`fallback` pour essayer plusieurs mod√®les)
- `models`: Liste de mod√®les en fallback

### M√©thode 2: Model Settings (au niveau mod√®le)

Configurer le mod√®le avec des param√®tres permanents:

```typescript
const model = openrouter('openai/gpt-4o', {
  extraBody: {
    reasoning: { max_tokens: 10 },
    transforms: ['middle-out']
  }
})

// Tous les appels avec ce mod√®le utiliseront ces params
const result = await generateText({
  model,
  prompt: 'Hello'
})
```

### M√©thode 3: Factory Configuration (globale)

Configurer le provider avec des valeurs par d√©faut:

```typescript
import { createOpenRouter } from '@openrouter/ai-sdk-provider'

const openrouter = createOpenRouter({
  apiKey: process.env.OPENROUTER_API_KEY,
  extraBody: {
    reasoning: { max_tokens: 10 }
  },
  headers: {
    'HTTP-Referer': 'https://yourapp.com',
    'X-Title': 'Your App'
  }
})

// Tous les mod√®les cr√©√©s avec cette factory h√©ritent de la config
const model = openrouter('openai/gpt-4o')
```

---

## Anthropic Prompt Caching

OpenRouter supporte le prompt caching d'Anthropic pour r√©duire les co√ªts et la latence sur les prompts r√©p√©t√©s.

### Activation du cache

```typescript
import { generateText } from 'ai'

const largeSystemPrompt = `
  [Votre long prompt syst√®me ici - ex: documentation compl√®te]
  ${/* ... 10000+ tokens ... */}
`

const result = await generateText({
  model: openrouter('anthropic/claude-3-5-sonnet'),
  messages: [
    {
      role: 'system',
      content: [
        {
          type: 'text',
          text: largeSystemPrompt,
          providerOptions: {
            openrouter: {
              cacheControl: { type: 'ephemeral' }
            }
          }
        }
      ]
    },
    {
      role: 'user',
      content: 'Answer based on the documentation above'
    }
  ]
})
```

**Avantages**:
- ‚úÖ R√©duction des co√ªts (cache hits ~90% moins chers)
- ‚úÖ Latence r√©duite (pas de retraitement du prompt)
- ‚úÖ TTL automatique (5 minutes par d√©faut)

**Use cases**:
- Documentation technique longue
- Context de RAG important
- Exemples few-shot volumineux
- System prompts complexes

---

## Usage Tracking

**üéâ Bonne nouvelle** : OpenRouter fournit **TOUJOURS** des m√©triques d√©taill√©es sur l'utilisation et les co√ªts, que vous l'ayez explicitement demand√© ou non.

### Acc√®s aux m√©triques (generateText)

```typescript
const model = openrouter('openai/gpt-3.5-turbo')
// Pas besoin de { usage: { include: true } }

const result = await generateText({
  model,
  prompt: 'Write a haiku about AI'
})

// Acc√®s aux m√©triques - TOUJOURS disponibles
const usage = result.providerMetadata?.openrouter?.usage

console.log('Tokens utilis√©s:', usage.totalTokens)
console.log('Co√ªt total:', usage.cost)
console.log('Prompt tokens:', usage.promptTokens)
console.log('Completion tokens:', usage.completionTokens)
```

### Acc√®s aux m√©triques (streamText) - IMPORTANT

**‚ö†Ô∏è Avec streaming, utilisez le callback `onFinish`** :

```typescript
const model = openrouter('openai/gpt-3.5-turbo')

// Variable pour capturer les m√©triques
let usage: any = null

const result = streamText({
  model,
  prompt: 'Write a haiku about AI',
  onFinish: (event) => {
    // ‚úÖ Les m√©triques sont disponibles ici
    usage = event.providerMetadata?.openrouter?.usage
  }
})

// Consommer le stream
for await (const chunk of result.textStream) {
  process.stdout.write(chunk)
}

// Utiliser les m√©triques apr√®s le stream
console.log('Co√ªt:', usage?.cost)
console.log('Tokens:', usage?.totalTokens)
```

### Structure des m√©triques

```typescript
interface OpenRouterUsage {
  totalTokens: number              // Total tokens utilis√©s
  promptTokens: number             // Tokens du prompt
  completionTokens: number         // Tokens g√©n√©r√©s
  cost: number                     // Co√ªt total en USD

  // D√©tails suppl√©mentaires
  promptTokensDetails: {
    cachedTokens: number           // Tokens en cache (Anthropic)
  }
  completionTokensDetails: {
    reasoningTokens: number        // Tokens de raisonnement (O1)
  }
}
```

**Note** : Toutes ces m√©triques sont **automatiquement incluses** dans chaque r√©ponse OpenRouter.

### Monitoring des co√ªts en temps r√©el

```typescript
async function generateWithBudget(prompt: string, maxCost: number) {
  const model = openrouter('openai/gpt-4o')

  const result = await generateText({ model, prompt })

  const cost = result.providerMetadata?.openrouter?.usage?.cost || 0

  if (cost > maxCost) {
    throw new Error(`Cost ${cost} exceeds budget ${maxCost}`)
  }

  return { result, cost }
}
```

### Monitoring avec streaming

```typescript
class StreamCostMonitor {
  private totalCost = 0
  private totalTokens = 0

  async streamWithTracking(model: any, prompt: string) {
    const result = streamText({
      model,
      prompt,
      onFinish: (event) => {
        const usage = event.providerMetadata?.openrouter?.usage
        if (usage) {
          this.totalCost += usage.cost || 0
          this.totalTokens += usage.totalTokens || 0
          console.log(`Request: ${usage.cost}$ (${usage.totalTokens} tokens)`)
          console.log(`Total: ${this.totalCost}$ (${this.totalTokens} tokens)`)
        }
      }
    })

    for await (const chunk of result.textStream) {
      process.stdout.write(chunk)
    }
  }

  getStats() {
    return {
      totalCost: this.totalCost,
      totalTokens: this.totalTokens
    }
  }
}
```

---

## Int√©gration avec @fondation-io/ai-sdk-tools

### Avec @fondation-io/agents

```typescript
import { Agent } from '@fondation-io/agents'
import { openrouter } from '@openrouter/ai-sdk-provider'

// Agent financier avec Claude
const financialAgent = new Agent({
  name: 'Financial Advisor',
  model: openrouter('anthropic/claude-3-5-sonnet'),
  instructions: 'You are a financial advisor',
  tools: {
    analyzeBurnRate: /* ... */,
    calculateROI: /* ... */
  }
})

// Agent technique avec GPT-4
const technicalAgent = new Agent({
  name: 'Technical Support',
  model: openrouter('openai/gpt-4o'),
  instructions: 'You are a technical expert',
  tools: {
    debugCode: /* ... */,
    reviewPR: /* ... */
  },
  handoffs: [financialAgent]
})

// Utilisation
const result = await technicalAgent.generate({
  prompt: 'Analyze our burn rate and suggest optimizations'
})
```

### Avec @fondation-io/cache

```typescript
import { cached } from '@fondation-io/cache'
import { openrouter } from '@openrouter/ai-sdk-provider'
import { tool } from 'ai'

// Tool cach√© avec OpenRouter
const weatherTool = cached(tool({
  description: 'Get weather information',
  parameters: z.object({ city: z.string() }),
  execute: async ({ city }) => {
    const { text } = await generateText({
      model: openrouter('openai/gpt-3.5-turbo'),
      prompt: `What's the weather in ${city}?`
    })
    return text
  }
}), {
  ttl: 30 * 60 * 1000 // Cache 30 minutes
})
```

### Avec @fondation-io/artifacts

```typescript
import { artifact } from '@fondation-io/artifacts'
import { openrouter } from '@openrouter/ai-sdk-provider'
import { tool } from 'ai'
import { z } from 'zod'

// Artifact pour charts
const chartArtifact = artifact('chart', z.object({
  title: z.string(),
  data: z.array(z.object({
    label: z.string(),
    value: z.number()
  }))
}))

// Tool qui g√©n√®re un chart
const analysisTool = tool({
  description: 'Analyze financial data',
  parameters: z.object({ metric: z.string() }),
  execute: async function* ({ metric }) {
    const chart = chartArtifact.stream({
      title: `Analysis: ${metric}`,
      data: []
    })

    // G√©n√©ration avec Gemini (bon pour l'analyse)
    const { text } = await generateText({
      model: openrouter('google/gemini-pro-1.5'),
      prompt: `Analyze ${metric} and provide data points`
    })

    // Parser et compl√©ter l'artifact
    await chart.complete({
      title: `Analysis: ${metric}`,
      data: parseDataPoints(text)
    })

    yield { text: 'Analysis complete', forceStop: true }
  }
})
```

### Avec @fondation-io/memory

```typescript
import { Agent } from '@fondation-io/agents'
import { UpstashProvider } from '@fondation-io/memory'
import { openrouter } from '@openrouter/ai-sdk-provider'
import { Redis } from '@upstash/redis'

const agent = new Agent({
  name: 'Assistant',
  model: openrouter('anthropic/claude-3-5-sonnet'),
  instructions: 'You are a helpful assistant with long-term memory',
  memory: {
    provider: new UpstashProvider({
      redis: Redis.fromEnv()
    }),
    workingMemory: {
      enabled: true,
      scope: 'user'
    },
    history: {
      enabled: true
    }
  }
})
```

---

## Strat√©gies de fallback

OpenRouter permet de configurer des fallbacks automatiques entre mod√®les.

### Fallback simple

```typescript
const result = await generateText({
  model: openrouter('openai/gpt-4o'),
  prompt: 'Hello',
  providerOptions: {
    openrouter: {
      route: 'fallback',
      models: [
        'openai/gpt-4o',           // Essayer d'abord
        'anthropic/claude-3-5-sonnet', // Fallback 1
        'google/gemini-pro-1.5'    // Fallback 2
      ]
    }
  }
})
```

### Fallback avec budget

```typescript
const budget = {
  cheap: ['openai/gpt-3.5-turbo', 'meta-llama/llama-3.1-70b'],
  medium: ['openai/gpt-4o-mini', 'google/gemini-flash-1.5'],
  expensive: ['openai/gpt-4o', 'anthropic/claude-3-5-sonnet']
}

async function generateWithBudget(prompt: string, tier: keyof typeof budget) {
  return await generateText({
    model: openrouter(budget[tier][0]),
    prompt,
    providerOptions: {
      openrouter: {
        route: 'fallback',
        models: budget[tier]
      }
    }
  })
}
```

---

## Exemples d'architecture Next.js

### Route API avec OpenRouter

```typescript
// app/api/chat/route.ts
import { openrouter } from '@openrouter/ai-sdk-provider'
import { streamText } from 'ai'

export async function POST(req: Request) {
  const { messages } = await req.json()

  const result = streamText({
    model: openrouter('anthropic/claude-3-5-sonnet'),
    messages,
    temperature: 0.7,
    maxTokens: 2000
  })

  return result.toDataStreamResponse()
}
```

### Server Action avec usage tracking

```typescript
// app/actions/generate.ts
'use server'

import { openrouter } from '@openrouter/ai-sdk-provider'
import { generateText } from 'ai'

export async function generateRecipe(ingredients: string[]) {
  const model = openrouter('openai/gpt-4o', {
    usage: { include: true }
  })

  const result = await generateText({
    model,
    prompt: `Create a recipe using: ${ingredients.join(', ')}`
  })

  return {
    recipe: result.text,
    cost: result.providerMetadata?.openrouter?.usage?.cost || 0,
    tokens: result.providerMetadata?.openrouter?.usage?.total_tokens || 0
  }
}
```

### Streaming avec prompt caching

```typescript
// app/api/chat-with-docs/route.ts
import { openrouter } from '@openrouter/ai-sdk-provider'
import { streamText } from 'ai'

const DOCUMENTATION = `
  [Your large documentation here - 50k+ tokens]
`

export async function POST(req: Request) {
  const { messages } = await req.json()

  const result = streamText({
    model: openrouter('anthropic/claude-3-5-sonnet'),
    messages: [
      {
        role: 'system',
        content: [
          {
            type: 'text',
            text: DOCUMENTATION,
            providerOptions: {
              openrouter: {
                cacheControl: { type: 'ephemeral' }
              }
            }
          }
        ]
      },
      ...messages
    ]
  })

  return result.toDataStreamResponse()
}
```

---

## Patterns avanc√©s

### Multi-model routing

```typescript
import { openrouter } from '@openrouter/ai-sdk-provider'

async function routeToModel(task: string, complexity: 'low' | 'medium' | 'high') {
  const models = {
    low: openrouter('openai/gpt-3.5-turbo'),
    medium: openrouter('openai/gpt-4o-mini'),
    high: openrouter('anthropic/claude-3-5-sonnet')
  }

  const result = await generateText({
    model: models[complexity],
    prompt: task
  })

  return result
}
```

### Cost optimization

```typescript
import { openrouter } from '@openrouter/ai-sdk-provider'

class CostOptimizedGeneration {
  private budget: number
  private spent: number = 0

  constructor(budgetUSD: number) {
    this.budget = budgetUSD
  }

  async generate(prompt: string, preferredModel: string) {
    if (this.spent >= this.budget) {
      throw new Error('Budget exceeded')
    }

    const model = openrouter(preferredModel, {
      usage: { include: true }
    })

    const result = await generateText({ model, prompt })

    const cost = result.providerMetadata?.openrouter?.usage?.cost || 0
    this.spent += cost

    console.log(`Spent: $${this.spent.toFixed(4)} / $${this.budget}`)

    return result
  }

  getRemainingBudget() {
    return this.budget - this.spent
  }
}

// Usage
const generator = new CostOptimizedGeneration(1.00) // $1 budget
await generator.generate('Task 1', 'openai/gpt-4o')
await generator.generate('Task 2', 'openai/gpt-3.5-turbo')
```

### Smart model selection

```typescript
async function smartModelSelection(prompt: string) {
  const promptLength = prompt.length
  const estimatedTokens = Math.ceil(promptLength / 4)

  let modelId: string

  if (estimatedTokens < 500) {
    modelId = 'openai/gpt-3.5-turbo' // Cheap for short prompts
  } else if (estimatedTokens < 2000) {
    modelId = 'openai/gpt-4o-mini' // Medium for medium prompts
  } else {
    modelId = 'google/gemini-pro-1.5' // Long context window
  }

  console.log(`Selected ${modelId} for ${estimatedTokens} estimated tokens`)

  return await generateText({
    model: openrouter(modelId),
    prompt
  })
}
```

---

## Comparaison des mod√®les

### Performance vs Co√ªt (r√©sultats tests r√©els)

**üí° R√©sultats de tests r√©els** (haiku TypeScript, ~30 tokens moyens) :

| Mod√®le | Co√ªt/requ√™te | Performance | Use case id√©al |
|--------|--------------|-------------|----------------|
| `openai/gpt-4o-mini` | **$0.000013** ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Meilleur rapport qualit√©/prix |
| `anthropic/claude-3-haiku` | $0.000030 | ‚≠ê‚≠ê‚≠ê‚≠ê | Rapide, pr√©cis |
| `openai/gpt-3.5-turbo` | $0.000032 | ‚≠ê‚≠ê‚≠ê | Chat basique |
| `google/gemini-flash-1.5` | ~$0.000035 | ‚≠ê‚≠ê‚≠ê‚≠ê | Long context, multimodal |
| `openai/gpt-4o` | ~$0.000150 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Production critique |
| `anthropic/claude-3-5-sonnet` | ~$0.000300 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Analyse complexe, code |

**üèÜ Recommandation** : Pour la plupart des cas d'usage, `openai/gpt-4o-mini` offre le meilleur rapport qualit√©/prix.

### Capacit√©s sp√©ciales

| Mod√®le | Tools | Vision | Long Context | Streaming |
|--------|-------|--------|--------------|-----------|
| `openai/gpt-4o` | ‚úÖ | ‚úÖ | 128k | ‚úÖ |
| `anthropic/claude-3-5-sonnet` | ‚úÖ | ‚úÖ | 200k | ‚úÖ |
| `google/gemini-pro-1.5` | ‚úÖ | ‚úÖ | 2M | ‚úÖ |
| `openai/o1-preview` | ‚ùå | ‚ùå | 128k | ‚úÖ |
| `meta-llama/llama-3.3-70b` | ‚úÖ | ‚ùå | 128k | ‚úÖ |

---

## Debugging et monitoring

### Logs d√©taill√©s

```typescript
import { openrouter } from '@openrouter/ai-sdk-provider'
import { generateText } from 'ai'

const result = await generateText({
  model: openrouter('openai/gpt-4o', {
    usage: { include: true }
  }),
  prompt: 'Hello',
  onFinish: ({ text, usage, finishReason }) => {
    console.log('Generated:', text)
    console.log('Finish reason:', finishReason)
    console.log('Usage:', usage)
  }
})

// Provider metadata
console.log('OpenRouter metadata:', result.providerMetadata?.openrouter)
```

### Error handling

```typescript
import { openrouter } from '@openrouter/ai-sdk-provider'
import { generateText } from 'ai'

try {
  const result = await generateText({
    model: openrouter('openai/gpt-4o'),
    prompt: 'Hello'
  })
} catch (error) {
  if (error.message?.includes('rate limit')) {
    console.error('Rate limit exceeded, retry with exponential backoff')
  } else if (error.message?.includes('insufficient credits')) {
    console.error('Insufficient credits, add funds to OpenRouter')
  } else {
    console.error('Unknown error:', error)
  }
}
```

### Request monitoring

```typescript
class RequestMonitor {
  private requests: Array<{
    timestamp: Date
    model: string
    tokens: number
    cost: number
  }> = []

  async track(modelId: string, prompt: string) {
    const model = openrouter(modelId, {
      usage: { include: true }
    })

    const result = await generateText({ model, prompt })

    const usage = result.providerMetadata?.openrouter?.usage

    this.requests.push({
      timestamp: new Date(),
      model: modelId,
      tokens: usage?.total_tokens || 0,
      cost: usage?.cost || 0
    })

    return result
  }

  getStats() {
    const totalCost = this.requests.reduce((sum, r) => sum + r.cost, 0)
    const totalTokens = this.requests.reduce((sum, r) => sum + r.tokens, 0)

    return {
      requestCount: this.requests.length,
      totalCost,
      totalTokens,
      averageCostPerRequest: totalCost / this.requests.length,
      requests: this.requests
    }
  }
}
```

---

## Best Practices

### 1. S√©curit√©

```typescript
// ‚úÖ GOOD: Utiliser variables d'environnement
const openrouter = createOpenRouter({
  apiKey: process.env.OPENROUTER_API_KEY
})

// ‚ùå BAD: Hardcoder la cl√©
const openrouter = createOpenRouter({
  apiKey: 'sk-or-v1-abc123...' // NEVER DO THIS
})
```

### 2. Error handling

```typescript
// ‚úÖ GOOD: G√©rer les erreurs sp√©cifiquement
try {
  const result = await generateText({ model, prompt })
} catch (error) {
  if (isRateLimitError(error)) {
    await retryWithBackoff()
  } else if (isInsufficientCreditsError(error)) {
    await notifyAdmin()
  }
  throw error
}

// ‚ùå BAD: Ignorer les erreurs
const result = await generateText({ model, prompt }).catch(() => null)
```

### 3. Cost monitoring (streaming)

```typescript
// ‚úÖ GOOD: Utiliser onFinish pour capturer les m√©triques
const result = streamText({
  model: openrouter('openai/gpt-4o'),
  prompt: '...',
  onFinish: (event) => {
    const cost = event.providerMetadata?.openrouter?.usage?.cost
    console.log(`Co√ªt: $${cost}`)
    analytics.track('llm_cost', { cost })
  }
})

// ‚ùå BAD: Essayer d'acc√©der directement apr√®s le stream
const result = streamText({ model, prompt })
for await (const chunk of result.textStream) {}
const cost = result.providerMetadata?.openrouter?.usage?.cost // undefined!
```

### 4. Model selection

```typescript
// ‚úÖ GOOD: Choisir le bon mod√®le pour la t√¢che
const simpleTask = openrouter('openai/gpt-3.5-turbo')
const complexTask = openrouter('anthropic/claude-3-5-sonnet')

// ‚ùå BAD: Utiliser GPT-4o pour tout
const model = openrouter('openai/gpt-4o')
```

### 5. Acc√®s aux m√©triques en streaming

```typescript
// ‚úÖ GOOD: Toujours capturer via onFinish
let usage: any = null

const result = streamText({
  model,
  prompt: '...',
  onFinish: (event) => {
    usage = event.providerMetadata?.openrouter?.usage
  }
})

for await (const chunk of result.textStream) {
  process.stdout.write(chunk)
}

console.log('Co√ªt final:', usage?.cost)

// ‚ùå BAD: Essayer d'acc√©der directement
const result = streamText({ model, prompt })
for await (const chunk of result.textStream) {}
// result.providerMetadata est undefined en streaming
```

---

## Ressources

### Documentation
- **OpenRouter**: https://openrouter.ai/docs
- **AI SDK v5**: https://sdk.vercel.ai/docs
- **Provider GitHub**: https://github.com/OpenRouterTeam/ai-sdk-provider

### Outils
- **Liste des mod√®les**: https://openrouter.ai/models
- **Pricing**: https://openrouter.ai/models (voir colonne prix)
- **Playground**: https://openrouter.ai/playground

### Support
- **Discord OpenRouter**: https://discord.gg/openrouter
- **Issues GitHub**: https://github.com/OpenRouterTeam/ai-sdk-provider/issues

---

## Changelog et versions

### v5.x (AI SDK v5)
- ‚úÖ Support complet AI SDK v5
- ‚úÖ Prompt caching Anthropic
- ‚úÖ Usage tracking d√©taill√©
- ‚úÖ Provider options
- ‚úÖ Fallback routes

### v4.x (AI SDK v4 - legacy)
- Support AI SDK v4
- Installation via `@ai-sdk-v4` tag

---

**Derni√®re mise √† jour**: Document bas√© sur @openrouter/ai-sdk-provider pour AI SDK v5 (d√©cembre 2024)

---

## D√©couvertes importantes (tests r√©els)

### 1. Les m√©triques sont TOUJOURS disponibles

Contrairement √† ce que sugg√®re la documentation, **OpenRouter retourne automatiquement les m√©triques d'usage** pour chaque requ√™te, sans configuration sp√©ciale.

```typescript
// ‚ùå PAS N√âCESSAIRE
const model = openrouter('...', { usage: { include: true } })

// ‚úÖ SUFFISANT - Les m√©triques sont toujours l√†
const model = openrouter('...')
```

### 2. Streaming : utilisez onFinish

En mode streaming, `providerMetadata` n'est **pas accessible** directement sur l'objet result. Vous **devez** utiliser le callback `onFinish`.

```typescript
// ‚úÖ CORRECT
const result = streamText({
  model,
  prompt: '...',
  onFinish: (event) => {
    const usage = event.providerMetadata?.openrouter?.usage
    console.log('Tokens:', usage?.totalTokens)
    console.log('Co√ªt:', usage?.cost)
  }
})

for await (const chunk of result.textStream) {
  // Stream le texte
}
```

### 3. GPT-4o-mini est le plus √©conomique

D'apr√®s nos tests r√©els (haiku TypeScript, ~30 tokens) :

```
openai/gpt-4o-mini        : $0.000013 (le moins cher)
anthropic/claude-3-haiku  : $0.000030 (2.3x plus cher)
openai/gpt-3.5-turbo      : $0.000032 (2.5x plus cher)
```

Pour la plupart des cas d'usage, GPT-4o-mini offre le meilleur rapport qualit√©/prix.

### 4. M√©triques d√©taill√©es incluses

OpenRouter retourne des m√©triques avanc√©es :

```typescript
{
  promptTokens: 13,
  completionTokens: 14,
  totalTokens: 27,
  cost: 0.000028,
  
  promptTokensDetails: {
    cachedTokens: 0           // Anthropic caching
  },
  completionTokensDetails: {
    reasoningTokens: 0        // O1 reasoning tokens
  }
}
```

### 5. Script de test disponible

Testez vous-m√™me le tracking d'usage :

```bash
export OPENROUTER_API_KEY=sk-or-v1-...
bun run docs/test-openrouter-streaming-usage.ts
```

Le script ex√©cute 4 tests :
1. ‚úÖ Streaming avec tracking
2. ‚úÖ Streaming sans tracking (m√©triques quand m√™me pr√©sentes !)
3. ‚úÖ Comparaison de co√ªts entre mod√®les
4. ‚úÖ Monitoring des co√ªts sur plusieurs appels

